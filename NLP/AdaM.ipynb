{"cells":[{"cell_type":"markdown","id":"0c4f2718","metadata":{"id":"0c4f2718"},"source":["# **Важно!**\n","\n","Домашнее задание состоит из нескольких задач, которые вам нужно решить.\n","*   Баллы выставляются по принципу выполнено/невыполнено.\n","*   За каждую выполненую задачу вы получаете баллы (количество баллов за задание указано в скобках).\n","\n","**Инструкция выполнения:** Выполните задания в этом же ноутбуке (места под решения **КАЖДОЙ** задачи обозначены как **#НАЧАЛО ВАШЕГО РЕШЕНИЯ** и **#КОНЕЦ ВАШЕГО РЕШЕНИЯ**)\n","\n","**Как отправить задание на проверку:** Вам необходимо сохранить ваше решение в данном блокноте и отправить итоговый **файл .IPYNB** на учебной платформе в **стандартную форму сдачи домашнего задания.**\n","\n","**Срок проверки преподавателем:** домашнее задание проверяется **в течение 3 дней после дедлайна сдачи** с предоставлением обратной связи\n","\n","# **Прежде чем проверять задания:**\n","\n","1. Перезапустите **ядро (restart the kernel)**: в меню, выбрать **Ядро (Kernel)**\n","→ **Перезапустить (Restart)**\n","2. Затем **Выполнить** **все ячейки (run all cells)**: в меню, выбрать **Ячейка (Cell)**\n","→ **Запустить все (Run All)**.\n","\n","После ячеек с заданием следуют ячейки с проверкой **с помощью assert.**\n","\n","Если в коде есть ошибки, assert выведет уведомление об ошибке.\n","\n","Если в коде нет ошибок, assert отработает без вывода дополнительной информации."]},{"cell_type":"markdown","id":"bafc5084","metadata":{"id":"bafc5084"},"source":["---"]},{"cell_type":"markdown","id":"90c0530b-e4d7-4d40-a33e-fce6fc719390","metadata":{"id":"90c0530b-e4d7-4d40-a33e-fce6fc719390"},"source":["Adam (Adaptive Moment Estimation) - это оптимизатор, используемый в машинном обучении для обновления параметров модели в процессе обучения. Этот оптимизатор является комбинацией двух других популярных оптимизаторов: градиентного спуска с моментом и адаптивного градиентного спуска.\n","\n","Основные преимущества Adam:\n","\n","1. **Адаптивные скорости обучения:** Adam автоматически адаптирует скорость обучения для каждого параметра на основе истории градиентов. Это позволяет эффективно обучать модели с разными скоростями обучения для разных параметров.\n","\n","2. **Момент:** Adam включает в себя момент (momentum), что помогает ускорить обучение и уменьшить возможность застревания в локальных минимумах.\n","\n","3. **Скользящее среднее:** Adam поддерживает скользящее среднее градиентов и их квадратов, что улучшает стабильность и сходимость оптимизации.\n","\n","4. **Регуляризация:** В Adam можно использовать L2-регуляризацию на параметры модели.\n","\n","Общая формула обновления параметров с использованием оптимизатора Adam выглядит следующим образом:\n","\n","```\n","m_t = beta1 * m_{t-1} + (1 - beta1) * g_t  # Экспоненциальное скользящее среднее градиентов\n","v_t = beta2 * v_{t-1} + (1 - beta2) * g_t^2  # Экспоненциальное скользящее среднее квадратов градиентов\n","m_hat_t = m_t / (1 - beta1^t)  # Коррекция момента\n","v_hat_t = v_t / (1 - beta2^t)  # Коррекция среднего квадрата\n","theta_t = theta_{t-1} - alpha * m_hat_t / (sqrt(v_hat_t) + epsilon)  # Обновление параметров модели\n","```\n","\n","Где:\n","- `g_t` - градиент функции потерь по параметрам модели в момент времени `t`.\n","- `m_t` и `v_t` - скользящие средние градиентов и квадратов градиентов соответственно.\n","- `m_hat_t` и `v_hat_t` - скорректированные значения скользящих средних и квадратов градиентов.\n","- `alpha` - скорость обучения (learning rate).\n","- `beta1` и `beta2` - коэффициенты сглаживания для скользящих средних.\n","- `epsilon` - небольшая константа, добавленная для численной стабильности.\n","\n","Adam является одним из наиболее популярных и эффективных оптимизаторов для обучения глубоких нейронных сетей, и он широко используется в практике машинного обучения и глубокого обучения."]},{"cell_type":"code","execution_count":null,"id":"b1362be6-7798-4b54-b699-d6cf48225a09","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"7dd730f5bf91fbba5e436edd9af9a14b","grade":false,"grade_id":"cell-2c49a34fff1c4c7c","locked":false,"schema_version":3,"solution":true,"task":false},"id":"b1362be6-7798-4b54-b699-d6cf48225a09"},"outputs":[],"source":["import numpy as np\n","\n","class AdamOptimizer:\n","    \"\"\"\n","    Задание 1 (1 балл):\n","\n","    Напишите метод `__init__` для класса `AdamOptimizer`.\n","\n","    **Описание задания:**\n","\n","    Вы должны создать конструктор класса `AdamOptimizer`, который инициализирует параметры оптимизатора Adam.\n","\n","    **Параметры конструктора:**\n","    1. `learning_rate` (по умолчанию 0.001): Скорость обучения (learning rate) оптимизатора Adam.\n","    2. `beta1` (по умолчанию 0.9): Коэффициент для экспоненциального скользящего среднего первого порядка.\n","    3. `beta2` (по умолчанию 0.999): Коэффициент для экспоненциального скользящего среднего второго порядка.\n","    4. `epsilon` (по умолчанию 1e-8): Малое число для предотвращения деления на ноль.\n","\n","    **Поля экземпляра класса:**\n","    1. `learning_rate`: Скорость обучения.\n","    2. `beta1`: Значение `beta1`.\n","    3. `beta2`: Значение `beta2`.\n","    4. `epsilon`: Значение `epsilon`.\n","    5. `momentums`: Инициализируется как `None`. Здесь будет храниться экспоненциальное скользящее среднее первого порядка.\n","    6. `running_squared_gradients`: Инициализируется как `None`. Здесь будет храниться экспоненциальное скользящее среднее второго порядка.\n","    7. `t`: Инициализируется как 0. Это счетчик шагов оптимизации.\n","\n","    **Пример использования:**\n","\n","    # Создаем объект AdamOptimizer с заданными параметрами\n","    optimizer = AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8)\n","    \"\"\"\n","    def __init__(self, learning_rate=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-8):\n","    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n","        self.learning_rate = learning_rate\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.epsilon = epsilon\n","        self.momentums = None\n","        self.running_squared_gradients = None\n","        self.t = 0\n","    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n","\n","    \"\"\"\n","    Задание 2 (2 балла):\n","\n","    **Задача:** Напишите метод `initialize` для класса `AdamOptimizer`.\n","\n","    **Описание задания:**\n","\n","    Вам необходимо реализовать метод `initialize`, который будет выполнять инициализацию моментов первого и второго порядка для каждого параметра оптимизатора Adam.\n","\n","    **Параметры метода:**\n","    1. `parameters` - Список, содержащий параметры, которые нужно инициализировать.\n","\n","    **Действия метода:**\n","    1. Проверьте, что переданный список `parameters` не пустой. Если список пустой, выбросьте исключение с текстом \"Необходимо передать параметры\".\n","\n","    2. Для каждого параметра в списке `parameters` создайте массивы нулей с такой же формой и типом данных, как у соответствующего параметра. Эти массивы будут использоваться для хранения моментов первого и второго порядка.\n","\n","    3. Присвойте полученные массивы нулей атрибутам `momentums` и `running_squared_gradients` класса `AdamOptimizer`.\n","\n","    **Пример использования:**\n","    # Создаем объект AdamOptimizer\n","    optimizer = AdamOptimizer(learning_rate=0.001)\n","\n","    # Инициализируем моменты для списка параметров\n","    parameters = [np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n","    optimizer.initialize(parameters)\n","    \"\"\"\n","    def initialize(self, parameters):\n","    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n","        if not parameters:\n","            raise Exception(\"Необходимо передать параметры\")\n","\n","        self.momentums, self.running_squared_gradients = [], []\n","        for i in range(len(parameters)):\n","            self.momentums.append(np.zeros_like(parameters[i]))\n","            self.running_squared_gradients.append(np.zeros_like(parameters[i]))\n","\n","        return self.momentums, self.running_squared_gradients\n","    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n","\n","    \"\"\"\n","    Задание 3 (4 балла):\n","\n","    **Задача:** Напишите метод `update` для класса `AdamOptimizer`.\n","\n","    **Описание задания:**\n","\n","    Вам необходимо реализовать метод `update`, который будет выполнять обновление параметров с использованием оптимизатора Adam.\n","\n","    **Параметры метода:**\n","    1. `parameters` - Список, содержащий параметры модели, которые нужно обновить.\n","    2. `gradients` - Список, содержащий градиенты параметров, вычисленные на текущем шаге оптимизации.\n","\n","    **Действия метода:**\n","    1. Проверьте атрибут `momentums` оптимизатора на равенство `None`.\n","    Если `momentums` равен `None`, вызовите self.initialize(parameters)\n","\n","    2. Проверьте, что список `gradients` не пустой.\n","    Если `gradients` пустой, выбросьте исключение с текстом \"Необходимо передать непустой градиент\".\n","\n","    3. Увеличьте значение атрибута `t` на 1.\n","\n","    4. Вычислите скорость обучения `learning_rate_t` на текущем шаге, используя формулу Adam.\n","    learning_rate_t = self.learning_rate * np.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t)\n","\n","    5. Для каждого параметра в списке `parameters` выполните следующие шаги:\n","       - Обновите момент первого порядка (экспоненциальное скользящее среднее градиента).\n","       self.momentums[i] = self.beta1 * self.momentums[i] + (1 - self.beta1) * gradients[i]\n","\n","       - Обновите момент второго порядка (экспоненциальное скользящее среднее квадрата градиента).\n","       self.running_squared_gradients[i] = self.beta2 * self.running_squared_gradients[i] + \\\n","                                                (1 - self.beta2) * gradients[i] ** 2\n","\n","       - Вычислите скорость обучения `learning_rate_i` для данного параметра, используя формулу Adam.\n","       learning_rate_i = learning_rate_t / (np.sqrt(self.running_squared_gradients[i]) + self.epsilon)\n","\n","       - Обновите значение параметра, учитывая моменты и скорость обучения.\n","       parameters[i] -= learning_rate_i * self.momentums[i]\n","\n","    **Пример использования:**\n","    # Создаем объект AdamOptimizer с заданными параметрами\n","    optimizer = AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8)\n","\n","    # Инициализируем моменты для списка параметров\n","    parameters = [np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n","    optimizer.initialize(parameters)\n","\n","    # Вычисляем градиенты\n","    gradients = [np.array([0.1, 0.2]), np.array([-0.3, -0.4])]\n","\n","    # Обновляем параметры с использованием метода update\n","    optimizer.update(parameters, gradients)\n","    \"\"\"\n","    def update(self, parameters, gradients):\n","    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n","        if self.momentums == None:\n","            self.initialize(parameters)\n","        if not gradients:\n","            raise Exception(\"Необходимо передать непустой градиент\")\n","\n","        self.t += 1\n","        learning_rate_t = self.learning_rate * np.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t)\n","        for i in range(len(parameters)):\n","            self.momentums[i] = self.beta1 * self.momentums[i] + (1 - self.beta1) * gradients[i]\n","            self.running_squared_gradients[i] = self.beta2 * self.running_squared_gradients[i] + \\\n","                                                (1 - self.beta2) * gradients[i] ** 2\n","            learning_rate_i = learning_rate_t / (np.sqrt(self.running_squared_gradients[i]) + self.epsilon)\n","            parameters[i] -= learning_rate_i * self.momentums[i]\n","\n","        return parameters\n","    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"]},{"cell_type":"code","execution_count":null,"id":"3835180d-12c9-4403-a4c1-16ff95d50797","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"0181c4c82c11f23aec2b31f98b87369c","grade":true,"grade_id":"cell-71db6cb4373cb3e2","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"3835180d-12c9-4403-a4c1-16ff95d50797"},"outputs":[],"source":["optimizer = AdamOptimizer()\n","assert optimizer.learning_rate == 0.001\n","assert optimizer.beta1 == 0.9\n","assert optimizer.beta2 == 0.999\n","assert optimizer.epsilon == 1e-8\n","assert optimizer.momentums is None\n","assert optimizer.running_squared_gradients is None\n","assert optimizer.t == 0\n","\n","optimizer = AdamOptimizer(learning_rate=0.01, beta1=0.95, beta2=0.99, epsilon=1e-6)\n","assert optimizer.learning_rate == 0.01\n","assert optimizer.beta1 == 0.95\n","assert optimizer.beta2 == 0.99\n","assert optimizer.epsilon == 1e-6\n","assert optimizer.momentums is None\n","assert optimizer.running_squared_gradients is None\n","assert optimizer.t == 0"]},{"cell_type":"code","execution_count":null,"id":"897f99e6-e408-4331-8b69-852a3f3a1379","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"4e524eacbd6ac9dfbc7ca7d91a826998","grade":true,"grade_id":"cell-99e8a66fa82093da","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"id":"897f99e6-e408-4331-8b69-852a3f3a1379"},"outputs":[],"source":["import numpy as np\n","\n","# обычное тестирование\n","optimizer = AdamOptimizer()\n","parameters = [np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n","optimizer.initialize(parameters)\n","\n","for i in range(len(parameters)):\n","    assert np.all(optimizer.momentums[i] == np.zeros_like(parameters[i]))\n","    assert np.all(optimizer.running_squared_gradients[i] == np.zeros_like(parameters[i]))\n","\n","# инициализация параметров не с помощью numpy\n","optimizer = AdamOptimizer()\n","parameters = [np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n","optimizer.initialize(parameters)\n","\n","for i in range(len(parameters)):\n","    assert np.all(optimizer.momentums[i] == np.zeros_like(parameters[i]))\n","    assert np.all(optimizer.running_squared_gradients[i] == np.zeros_like(parameters[i]))"]},{"cell_type":"code","execution_count":null,"id":"f31072f2-b2cf-4325-aad6-f4567dde3ee9","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"17affa39898a93250bc72ba758c75670","grade":true,"grade_id":"cell-7110baa3a3ec00fd","locked":true,"points":4,"schema_version":3,"solution":false,"task":false},"id":"f31072f2-b2cf-4325-aad6-f4567dde3ee9"},"outputs":[],"source":["\n","optimizer = AdamOptimizer(learning_rate=0.001)\n","parameters = [np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n","gradients = [np.array([0.1, 0.2]), np.array([-0.3, -0.4])]\n","\n","# Тестовый случай 1: Проверка корректности обновления параметров\n","optimizer.update(parameters, gradients)\n","\n","parameters\n","assert np.allclose(parameters[0], np.array([0.999, 1.999]), rtol=1e-5)\n","assert np.allclose(parameters[1], np.array([3.001, 4.001]), rtol=1e-5)\n","\n","# Тестовый случай 2: Проверка корректного инкремента t\n","assert optimizer.t == 1\n","\n","# Тестовый случай 3: Проверка правильности вычисления скорости обучения\n","assert np.isclose(optimizer.learning_rate, 0.001)\n","\n","# Тестовый случай 4: Проверка возникновения исключения при пустом градиенте\n","try:\n","    optimizer.update(parameters, [])\n","except Exception as e:\n","    assert str(e) == \"Необходимо передать непустой градиент\"\n","\n","# Тестовый случай 5: Проверка возникновения исключения при неинициализированном оптимизаторе\n","optimizer = AdamOptimizer(learning_rate=0.001)\n","try:\n","    optimizer.update([], [np.array([0.1, 0.2])])\n","except Exception as e:\n","    assert str(e) == \"Необходимо передать параметры\"\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}