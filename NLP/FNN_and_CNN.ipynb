{"cells":[{"cell_type":"markdown","id":"6e376fe8","metadata":{"id":"6e376fe8"},"source":["# **Важно!**\n","\n","Домашнее задание состоит из нескольких задач, которые вам нужно решить.\n","*   Баллы выставляются по принципу выполнено/невыполнено.\n","*   За каждую выполненую задачу вы получаете баллы (количество баллов за задание указано в скобках).\n","\n","**Инструкция выполнения:** Выполните задания в этом же ноутбуке (места под решения **КАЖДОЙ** задачи обозначены как **#НАЧАЛО ВАШЕГО РЕШЕНИЯ** и **#КОНЕЦ ВАШЕГО РЕШЕНИЯ**)\n","\n","**Как отправить задание на проверку:** Вам необходимо сохранить ваше решение в данном блокноте и отправить итоговый **файл .IPYNB** на учебной платформе в **стандартную форму сдачи домашнего задания.**\n","\n","**Срок проверки преподавателем:** домашнее задание проверяется **в течение 3 дней после дедлайна сдачи** с предоставлением обратной связи\n","\n","# **Прежде чем проверять задания:**\n","\n","1. Перезапустите **ядро (restart the kernel)**: в меню, выбрать **Ядро (Kernel)**\n","→ **Перезапустить (Restart)**\n","2. Затем **Выполнить** **все ячейки (run all cells)**: в меню, выбрать **Ячейка (Cell)**\n","→ **Запустить все (Run All)**.\n","\n","После ячеек с заданием следуют ячейки с проверкой **с помощью assert.**\n","\n","Если в коде есть ошибки, assert выведет уведомление об ошибке.\n","\n","Если в коде нет ошибок, assert отработает без вывода дополнительной информации."]},{"cell_type":"markdown","id":"7607aee0","metadata":{"id":"7607aee0"},"source":["---"]},{"cell_type":"markdown","id":"a9242bc2-22ed-4d5c-8365-c73775fbd9fb","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"c801267f0f5931eb73188cafe8e12204","grade":false,"grade_id":"cell-76404c8a4a924cb3","locked":true,"schema_version":3,"solution":false,"task":false},"id":"a9242bc2-22ed-4d5c-8365-c73775fbd9fb"},"source":["В этом домашнем задании мы исследуем применимость FNN и CNN для задач NLP.\n","Вам необходимо обучить нейронные сети на задаче классификации датасета 20newsgroups.\n","\n","Хорошее исследование на тему использования различных алгоритмов машинного обучения для классификации этого датасета представлено в статье https://arxiv.org/abs/1904.08067 (рекомендуется к прочтению). Также мы используем некоторые алгоритмы из репозитория к этой статье ( https://github.aiurs.co/kk7nc/Text_Classification/tree/master ).\n","\n","FNN настраиваются и обучаются достаточно легко.\n","Нейронные сети прямого распространения (FNN), могут использоваться в обработке естественного языка (NLP) для различных задач:\n","\n","1. **Классификация текста:**\n","   - Они эффективны в сценариях, где порядок слов или последовательность информации не является ключевым фактором для прогнозирования.\n","\n","2. **Классификация документов:**\n","   - Они могут хорошо справляться, когда структура документа не является сильно последовательной, и акцент делается на общем семантическом значении.\n","\n","3. **Извлечение признаков:**\n","   - FNN могут быть использованы как извлекатели признаков в более сложных моделях NLP. Обученные представления из скрытых слоев FNN могут служить полезными признаками для последующих слоев или моделей.\n","\n","4. **Информационный поиск:**\n","   - В задачах, связанных с информационным поиском, FNN могут использоваться для ранжирования документов по их релевантности заданному запросу.\n","\n","5. **Анализ тональности:**\n","   - FNN могут быть применены к задачам анализа тональности, где необходимо определить эмоциональную окраску в тексте.\n","\n","6. **Извлечение именованных сущностей (NER):**\n","   - Для простых задач извлечения именованных сущностей, где контекст вокруг сущностей не сильно зависит от последовательности, FNN могут использоваться для идентификации именованных сущностей в тексте.\n","\n","7. **Тематическое моделирование:**\n","   - FNN могут применяться в задачах тематического моделирования для выявления основных тем в коллекции документов.\n","\n","Хотя FNN в некоторых областях NLP были заменены более сложными моделями, такими как рекуррентные нейронные сети (RNN) и трансформеры, они всё ещё могут быть полезны в сценариях, где последовательность данных менее важна или когда важны ресурсы вычислений.\n","\n","Сверточные нейронные сети (CNN) не пользуются большим успехом в задачах обработки естественного языка (NLP), но могут применяться в некоторых задачах NLP:\n","\n","1. **Классификация текста:**\n","   - CNN могут использоваться для задач классификации текста, таких как определение тональности, тематическое маркирование, определение категорий, и\n","ых.\n","\n","2. **Распознавание именованных сущностей (NER):**\n","   - В задачах извлечения именованных сущностей CNN могут использоваться для распознавания и классификации именованных сущностей в тексте.\n","\n","3. **Извлечение признаков:**\n","   - CNN могут скак мощные инструменты для извлечения признаков из текстовых данных. Каждый сверточный слой может выделять различные аспекты текста.\n","\n","4. **Семантическая сегментация:**\n","   - В задачах семантической сегментации текста, где необходимо выделить семантические элементы или фразы, CNN могут быть применены для локализации и классификации различных частей текста.\n","\n","5. **Генерация текста:**\n","   - В некоторых сценариях CNN используются для генерации текста, хотя в этой области более популярными стали рекуррентные и трансформерные архитектуры.\n","\n","6. **Кластеризация тем:**\n","   - CNN могут применяться в задачах тематической кластеризации текстовых данных, где цель — группировать докумеы локати в данных.\n","\n","CNN практически не используются для задач NLP из-за сложностей в обучении и неоптимальном потреблении вычислительных ресурсов."]},{"cell_type":"code","execution_count":null,"id":"1aec6855-aa98-4b01-aa35-c390fc68ccad","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"ebb1da78e64eb56e0c5be759f8eeb44f","grade":false,"grade_id":"cell-82c8ccfa69595ad7","locked":true,"schema_version":3,"solution":false,"task":false},"id":"1aec6855-aa98-4b01-aa35-c390fc68ccad"},"outputs":[],"source":["import gc\n","\n","# Получаем список имен переменных в текущем пространстве имен\n","vars_before = globals().copy()"]},{"cell_type":"markdown","id":"84715ec0-3562-4ce3-948f-3bc5b17fbb09","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"671da4f38818125dbd65b9cb6a05b3ce","grade":false,"grade_id":"cell-71a7a543f5ab596e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"84715ec0-3562-4ce3-948f-3bc5b17fbb09"},"source":["# Задание 1. Полносвязные сети (FNN) (3 балла)\n","\n","FNN настраиваются достаточно просто. Возьмите код https://github.aiurs.co/kk7nc/Text_Classification/blob/master/code/DNN.py за основу своей модели FNN. Возможно, придется обновить некоторые импорты и зависимости.\n","\n","В результате обучения модели вы получите примерно такие результаты (достаточно 3 эпох):\n","\n","Epoch 1/3\n","89/89 - 79s - loss: 2.1687 - accuracy: 0.3464 - val_loss: 0.8187 - val_accuracy: 0.7840 - 79s/epoch - 885ms/ste\n","\n","p\n","Epoch 2/3\n","89/89 - 54s - loss: 0.3596 - accuracy: 0.8953 - val_loss: 0.6077 - val_accuracy: 0.8323 - 54s/epoch - 605ms/s\n","\n","tep\n","Epoch 3/3\n","89/89 - 54s - loss: 0.0717 - accuracy: 0.9838 - val_loss: 0.6099 - val_accuracy: 0.8452 - 54s/epoch - 603ms/step\n","236/236 [==============================] - 10s 41ms/step\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.77      0.80       319\n","           1       0.80      0.75      0.77       389\n","           2       0.69      0.79      0.74       394\n","           3       0.65      0.80      0.72       392\n","           4       0.86      0.82      0.84       385\n","           5       0.88      0.76      0.82       395\n","           6       0.88      0.86      0.87       390\n","           7       0.91      0.94      0.92       396\n","           8       0.98      0.93      0.96       398\n","           9       0.96      0.92      0.94       397\n","          10       0.93      0.97      0.95       399\n","          11       0.93      0.91      0.92       396\n","          12       0.77      0.77      0.77       393\n","          13       0.86      0.87      0.87       396\n","          14       0.89      0.93      0.91       394\n","          15       0.81      0.95      0.88       398\n","          16       0.79      0.87      0.83       364\n","          17       0.98      0.88      0.93       376\n","          18       0.80      0.63      0.71       310\n","          19       0.70      0.61      0.66       251\n","\n","    accuracy           \n","                    0.85      7532\n","   macro avg       0.8\n","   5      0.84      0.84      7532\n","weighted avg       0.8\n","5      0.85      0.85      7532\n"]},{"cell_type":"code","execution_count":null,"id":"e79b9f00-9bbe-4526-8afa-850d021c72a8","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"199e8a6572ba1722a9803aab8228faed","grade":true,"grade_id":"cell-ea7055f01c148e70","locked":false,"points":3,"schema_version":3,"solution":true,"task":false},"id":"e79b9f00-9bbe-4526-8afa-850d021c72a8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c8bdbd3-9db6-40c6-d340-93f35e95ea50","executionInfo":{"status":"ok","timestamp":1696189752686,"user_tz":-240,"elapsed":274604,"user":{"displayName":"Владимир Фролов","userId":"12452311964078337895"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tf-idf with 35000 features\n","Epoch 1/3\n","89/89 - 55s - loss: 2.6841 - accuracy: 0.1271 - val_loss: 1.7706 - val_accuracy: 0.3841 - 55s/epoch - 621ms/step\n","Epoch 2/3\n","89/89 - 49s - loss: 1.2572 - accuracy: 0.5407 - val_loss: 0.9662 - val_accuracy: 0.7094 - 49s/epoch - 551ms/step\n","Epoch 3/3\n","89/89 - 48s - loss: 0.5407 - accuracy: 0.8110 - val_loss: 0.8030 - val_accuracy: 0.7703 - 48s/epoch - 537ms/step\n","236/236 [==============================] - 10s 44ms/step\n","              precision    recall  f1-score   support\n","\n","           0       0.65      0.79      0.72       319\n","           1       0.68      0.60      0.64       389\n","           2       0.78      0.57      0.65       394\n","           3       0.64      0.67      0.65       392\n","           4       0.76      0.74      0.75       385\n","           5       0.72      0.79      0.75       395\n","           6       0.67      0.83      0.74       390\n","           7       0.78      0.80      0.79       396\n","           8       0.92      0.92      0.92       398\n","           9       0.97      0.85      0.91       397\n","          10       0.92      0.95      0.94       399\n","          11       0.96      0.85      0.90       396\n","          12       0.56      0.75      0.64       393\n","          13       0.81      0.76      0.78       396\n","          14       0.91      0.83      0.87       394\n","          15       0.92      0.83      0.87       398\n","          16       0.64      0.88      0.74       364\n","          17       0.96      0.83      0.89       376\n","          18       0.72      0.56      0.63       310\n","          19       0.52      0.43      0.47       251\n","\n","    accuracy                           0.77      7532\n","   macro avg       0.77      0.76      0.76      7532\n","weighted avg       0.78      0.77      0.77      7532\n","\n"]}],"source":["# Задание 1. Полносвязные сети (FNN) (3 балла)\n","\n","# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n","from sklearn.datasets import fetch_20newsgroups\n","from keras.layers import  Dropout, Dense\n","from keras.models import Sequential\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","from sklearn import metrics\n","\n","def TFIDF(X_train, X_test,MAX_NB_WORDS=35000):\n","    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n","    X_train = vectorizer_x.fit_transform(X_train).toarray()\n","    X_test = vectorizer_x.transform(X_test).toarray()\n","    print(\"tf-idf with\",str(len(X_train[0])),\"features\")\n","    return (X_train,X_test)\n","\n","def Build_Model_DNN_Text(shape, nClasses, dropout=0.5):\n","    \"\"\"\n","    buildModel_DNN_Tex(shape, nClasses,dropout)\n","    Build Deep neural networks Model for text classification\n","    Shape is input feature space\n","    nClasses is number of classes\n","    \"\"\"\n","    model = Sequential()\n","    node = 512 # number of nodes\n","    nLayers = 4 # number of  hidden layer\n","\n","    model.add(Dense(node,input_dim=shape,activation='relu'))\n","    model.add(Dropout(dropout))\n","    for i in range(0,nLayers):\n","        model.add(Dense(node,input_dim=node,activation='relu'))\n","        model.add(Dropout(dropout))\n","    model.add(Dense(nClasses, activation='softmax'))\n","\n","    model.compile(loss='sparse_categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","\n","    return model\n","\n","newsgroups_train = fetch_20newsgroups(subset='train')\n","newsgroups_test = fetch_20newsgroups(subset='test')\n","X_train = newsgroups_train.data\n","X_test = newsgroups_test.data\n","y_train = newsgroups_train.target\n","y_test = newsgroups_test.target\n","\n","X_train_tfidf,X_test_tfidf = TFIDF(X_train,X_test)\n","model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], 20)\n","model_DNN.fit(X_train_tfidf, y_train,\n","                              validation_data=(X_test_tfidf, y_test),\n","                              epochs=3,\n","                              batch_size=128,\n","                              verbose=2)\n","\n","predicted = model_DNN.predict(X_test_tfidf)\n","predicted = predicted.argmax(axis=1)\n","\n","print(metrics.classification_report(y_test, predicted))\n","# КОНЕЦ ВАШЕГО РЕШЕНИЯ"]},{"cell_type":"code","execution_count":null,"id":"97d98e12-04a0-48ce-bd09-218c42981028","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"931f20a40d2bb56ebb755270c5ffd26a","grade":false,"grade_id":"cell-61b09f002b4930c4","locked":true,"schema_version":3,"solution":false,"task":false},"id":"97d98e12-04a0-48ce-bd09-218c42981028","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d08ef695-3e06-4eec-cc01-27766ba81cbd","executionInfo":{"status":"ok","timestamp":1696189753176,"user_tz":-240,"elapsed":496,"user":{"displayName":"Владимир Фролов","userId":"12452311964078337895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2245"]},"metadata":{},"execution_count":3}],"source":["# Удаляем все переменные, созданные после vars_before\n","vars_after = globals().copy()\n","for var in vars_after:\n","    if var not in vars_before and var not in ['vars_after', 'vars_before']:\n","        del globals()[var]\n","\n","# Вызываем сборщик мусора для освобождения памяти\n","gc.collect()"]},{"cell_type":"markdown","id":"de58fafa-4a4e-489b-be74-b70ca3b9c153","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"0be3e2149e53596b337f731a85fe0a1d","grade":false,"grade_id":"cell-9247e1e83a07550e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"de58fafa-4a4e-489b-be74-b70ca3b9c153"},"source":["# Задание 2. Сверточные сети (CNN) (6 баллов)\n","\n","CNN настраиваются гораздо сложнее. Conv1D практически не обучаются (как пример, https://github.com/mireshghallah/20Newsgroups-Pytorch/blob/master/newsgroup-pytorch-git.ipynb )\n","\n","В этом задании мы увидим, что CNN гораздо сложнее обучаются на задачах NLP и менее эффективны.\n","\n","Возьмите код https://github.aiurs.co/kk7nc/Text_Classification/blob/master/code/CNN.py за основу своей модели СNN. Возможно, придется обновить некоторые импорты и зависимости.\n","\n","Скорее всего, вам придется существенно изменить параметры:\n","\n","- MAX_SEQUENCE_LENGTH\n","- использовать glove.6B.50d.txt\n","- EMBEDDING_DIM\n","- embedding_layer, установить trainable=False\n","- установить меньшее количество слоев\n","- установить меньшее количество фильтров\n","\n","В результате обучения модели вы получите примерно такие результаты (после 10 эпох):\n","\n","Epoch 1/10\n","177/177 - 152s - loss: 2.8562 - accuracy: 0.0855 - val_loss: 2.3600 - val_accuracy: 0.2063 - 152s/epoch - 859ms/step\n","Epoch 2/10\n","177/177 - 137s - loss: 2.1895 - accuracy: 0.2402 - val_loss: 1.9501 - val_accuracy: 0.3159 - 137s/epoch - 775ms/step\n","Epoch 3/10\n","177/177 - 133s - loss: 1.8319 - accuracy: 0.3584 - val_loss: 1.5831 - val_accuracy: 0.4559 - 133s/epoch - 751ms/step\n","Epoch 4/10\n","177/177 - 140s - loss: 1.5910 - accuracy: 0.4436 - val_loss: 1.4674 - val_accuracy: 0.5072 - 140s/epoch - 788ms/step\n","Epoch 5/10\n","177/177 - 138s - loss: 1.4495 - accuracy: 0.4966 - val_loss: 1.3952 - val_accuracy: 0.5303 - 138s/epoch - 780ms/step\n","Epoch 6/10\n","177/177 - 134s - loss: 1.3597 - accuracy: 0.5262 - val_loss: 1.3240 - val_accuracy: 0.5709 - 134s/epoch - 755ms/step\n","Epoch 7/10\n","177/177 - 141s - loss: 1.2472 - accuracy: 0.5661 - val_loss: 1.2545 - val_accuracy: 0.5981 - 141s/epoch - 794ms/step\n","Epoch 8/10\n","177/177 - 150s - loss: 1.1871 - accuracy: 0.5978 - val_loss: 1.2518 - val_accuracy: 0.5951 - 150s/epoch - 846ms/step\n","Epoch 9/10\n","177/177 - 137s - loss: 1.1105 - accuracy: 0.6244 - val_loss: 1.2663 - val_accuracy: 0.5920 - 137s/epoch - 774ms/step\n","Epoch 10/10\n","177/177 - 137s - loss: 1.0685 - accuracy: 0.6384 - val_loss: 1.2014 - val_accuracy: 0.6079 - 137s/epoch - 775ms/step\n","236/236 [==============================] - 13s 53ms/step\n","              precision    recall  f1-score   support\n","\n","           0       0.36      0.24      0.29       319\n","           1       0.45      0.67      0.53       389\n","           2       0.59      0.50      0.54       394\n","           3       0.37      0.61      0.46       392\n","           4       0.34      0.16      0.21       385\n","           5       0.75      0.31      0.44       395\n","           6       0.46      0.90      0.61       390\n","           7       0.70      0.74      0.72       396\n","           8       0.75      0.61      0.67       398\n","           9       0.92      0.71      0.80       397\n","          10       0.82      0.92      0.87       399\n","          11       0.72      0.72      0.72       396\n","          12       0.51      0.45      0.48       393\n","          13       0.90      0.72      0.80       396\n","          14       0.84      0.76      0.79       394\n","          15       0.52      0.91      0.66       398\n","          16       0.58      0.64      0.61       364\n","          17       0.77      0.84      0.80       376\n","          18       0.64      0.38      0.47       310\n","          19       0.26      0.06      0.09       251\n","\n","    accuracy                           0.61      7532\n","   macro avg       0.61      0.59      0.58      7532\n","weightevg       0.62      0.61      0.59      7532\n","\n","\n"]},{"cell_type":"code","source":["!wget http://nlp.stanford.edu/data/glove.6B.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9MuSsRMORBs","outputId":"48a7a34f-c2fc-4938-ed56-f2a591228b2d","executionInfo":{"status":"ok","timestamp":1696189913822,"user_tz":-240,"elapsed":160654,"user":{"displayName":"Владимир Фролов","userId":"12452311964078337895"}}},"id":"S9MuSsRMORBs","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-01 19:49:11--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2023-10-01 19:49:12--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2023-10-01 19:49:12--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.27MB/s    in 2m 39s  \n","\n","2023-10-01 19:51:52 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n"]}]},{"cell_type":"code","source":["!unzip glove*.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cce3VKeBaOVW","outputId":"e0d34141-d1c7-4f34-d821-0892edc8d6ca","executionInfo":{"status":"ok","timestamp":1696189953208,"user_tz":-240,"elapsed":39392,"user":{"displayName":"Владимир Фролов","userId":"12452311964078337895"}}},"id":"cce3VKeBaOVW","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"]}]},{"cell_type":"code","source":["!ls\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5POm-VkOaPYe","outputId":"95148a79-c042-4472-f2ed-a0033d2a7863","executionInfo":{"status":"ok","timestamp":1696189953208,"user_tz":-240,"elapsed":8,"user":{"displayName":"Владимир Фролов","userId":"12452311964078337895"}}},"id":"5POm-VkOaPYe","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n","glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n","/content\n"]}]},{"cell_type":"code","source":["# Задание 2. Сверточные сети (CNN) (6 баллов)\n","\n","# Скорее всего, вам придется существенно изменить параметры:\n","\n","# - MAX_SEQUENCE_LENGTH\n","# - использовать glove.6B.50d.txt\n","# - EMBEDDING_DIM\n","# - embedding_layer, установить trainable=False\n","# - установить меньшее количество слоев\n","# - установить меньшее количество фильтров\n","\n","# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n","from keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D\n","from keras.models import Sequential,Model\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","from sklearn import metrics\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.datasets import fetch_20newsgroups\n","from keras.layers import Concatenate\n","\n","def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=300):\n","    np.random.seed(7)\n","    #text = np.concatenate((X_train, X_test), axis=0)\n","    #text = np.array(text)\n","    text = X_train + X_test\n","    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","    tokenizer.fit_on_texts(text)\n","    sequences = tokenizer.texts_to_sequences(text)\n","    word_index = tokenizer.word_index\n","    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","    print('Found %s unique tokens.' % len(word_index))\n","    indices = np.arange(text.shape[0])\n","    # np.random.shuffle(indices)\n","    text = text[indices]\n","    print(text.shape)\n","    X_train = np.array(text[0:len(X_train), ])\n","    X_test = np.array(text[len(X_train):, ])\n","    embeddings_index = {}\n","    f = open(\"/content/glove.6B.50d.txt\", encoding=\"utf8\")\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        try:\n","            coefs = np.asarray(values[1:], dtype='float32')\n","        except:\n","            pass\n","        embeddings_index[word] = coefs\n","    f.close()\n","    print('Total %s word vectors.' % len(embeddings_index))\n","    return (X_train, X_test, word_index,embeddings_index)\n","\n","def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=300, EMBEDDING_DIM=50, dropout=0.5):\n","\n","    \"\"\"\n","        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n","        word_index in word index ,\n","        embeddings_index is embeddings index, look at data_helper.py\n","        nClasses is number of classes,\n","        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n","        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n","    \"\"\"\n","\n","    model = Sequential()\n","    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n","    for word, i in word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            # words not found in embedding index will be all-zeros.\n","            if len(embedding_matrix[i]) !=len(embedding_vector):\n","                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n","                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n","                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n","                exit(1)\n","\n","            embedding_matrix[i] = embedding_vector\n","\n","    embedding_layer = Embedding(len(word_index) + 1,\n","                                EMBEDDING_DIM,\n","                                weights=[embedding_matrix],\n","                                input_length=MAX_SEQUENCE_LENGTH,\n","                                trainable=True)\n","\n","    # applying a more complex convolutional approach\n","    convs = []\n","    filter_sizes = []\n","    layer = 3\n","    print(\"Filter  \",layer)\n","    for fl in range(0,layer):\n","        filter_sizes.append((fl+2))\n","\n","    node = 128\n","    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","    embedded_sequences = embedding_layer(sequence_input)\n","\n","    for fsz in filter_sizes:\n","        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n","        l_pool = MaxPooling1D(5)(l_conv)\n","        #l_pool = Dropout(0.25)(l_pool)\n","        convs.append(l_pool)\n","\n","    l_merge = Concatenate(axis=1)(convs)\n","    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n","    l_cov1 = Dropout(dropout)(l_cov1)\n","    l_pool1 = MaxPooling1D(5)(l_cov1)\n","    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n","    l_cov2 = Dropout(dropout)(l_cov2)\n","    l_pool2 = MaxPooling1D(2)(l_cov2)\n","    l_flat = Flatten()(l_pool2)\n","    l_dense = Dense(1024, activation='relu')(l_flat)\n","    l_dense = Dropout(dropout)(l_dense)\n","    l_dense = Dense(512, activation='relu')(l_dense)\n","    l_dense = Dropout(dropout)(l_dense)\n","    preds = Dense(nclasses, activation='softmax')(l_dense)\n","    model = Model(sequence_input, preds)\n","\n","    model.compile(loss='sparse_categorical_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['accuracy'])\n","\n","\n","\n","    return model\n","\n","newsgroups_train = fetch_20newsgroups(subset='train')\n","newsgroups_test = fetch_20newsgroups(subset='test')\n","X_train = newsgroups_train.data\n","X_test = newsgroups_test.data\n","y_train = newsgroups_train.target\n","y_test = newsgroups_test.target\n","\n","X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)\n","\n","\n","model_CNN = Build_Model_CNN_Text(word_index,embeddings_index, 20)\n","\n","\n","model_CNN.summary()\n","\n","model_CNN.fit(X_train_Glove, y_train,\n","                              validation_data=(X_test_Glove, y_test),\n","                              epochs=10,\n","                              batch_size=128,\n","                              verbose=2)\n","\n","predicted = model_CNN.predict(X_test_Glove)\n","\n","predicted = np.argmax(predicted, axis=1)\n","\n","\n","print(metrics.classification_report(y_test, predicted))\n","# КОНЕЦ ВАШЕГО РЕШЕНИЯ"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VQXfDMoJeK5r","outputId":"63f05901-ad19-4f51-dc39-cd48431690c2","executionInfo":{"status":"ok","timestamp":1696191094863,"user_tz":-240,"elapsed":1141659,"user":{"displayName":"Владимир Фролов","userId":"12452311964078337895"}}},"id":"VQXfDMoJeK5r","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 179209 unique tokens.\n","(18846, 300)\n","Total 400000 word vectors.\n","Filter   3\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 300)]                0         []                            \n","                                                                                                  \n"," embedding (Embedding)       (None, 300, 50)              8960500   ['input_1[0][0]']             \n","                                                                                                  \n"," conv1d (Conv1D)             (None, 299, 128)             12928     ['embedding[0][0]']           \n","                                                                                                  \n"," conv1d_1 (Conv1D)           (None, 298, 128)             19328     ['embedding[0][0]']           \n","                                                                                                  \n"," conv1d_2 (Conv1D)           (None, 297, 128)             25728     ['embedding[0][0]']           \n","                                                                                                  \n"," max_pooling1d (MaxPooling1  (None, 59, 128)              0         ['conv1d[0][0]']              \n"," D)                                                                                               \n","                                                                                                  \n"," max_pooling1d_1 (MaxPoolin  (None, 59, 128)              0         ['conv1d_1[0][0]']            \n"," g1D)                                                                                             \n","                                                                                                  \n"," max_pooling1d_2 (MaxPoolin  (None, 59, 128)              0         ['conv1d_2[0][0]']            \n"," g1D)                                                                                             \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 177, 128)             0         ['max_pooling1d[0][0]',       \n","                                                                     'max_pooling1d_1[0][0]',     \n","                                                                     'max_pooling1d_2[0][0]']     \n","                                                                                                  \n"," conv1d_3 (Conv1D)           (None, 173, 128)             82048     ['concatenate[0][0]']         \n","                                                                                                  \n"," dropout_5 (Dropout)         (None, 173, 128)             0         ['conv1d_3[0][0]']            \n","                                                                                                  \n"," max_pooling1d_3 (MaxPoolin  (None, 34, 128)              0         ['dropout_5[0][0]']           \n"," g1D)                                                                                             \n","                                                                                                  \n"," conv1d_4 (Conv1D)           (None, 30, 128)              82048     ['max_pooling1d_3[0][0]']     \n","                                                                                                  \n"," dropout_6 (Dropout)         (None, 30, 128)              0         ['conv1d_4[0][0]']            \n","                                                                                                  \n"," max_pooling1d_4 (MaxPoolin  (None, 15, 128)              0         ['dropout_6[0][0]']           \n"," g1D)                                                                                             \n","                                                                                                  \n"," flatten (Flatten)           (None, 1920)                 0         ['max_pooling1d_4[0][0]']     \n","                                                                                                  \n"," dense_6 (Dense)             (None, 1024)                 1967104   ['flatten[0][0]']             \n","                                                                                                  \n"," dropout_7 (Dropout)         (None, 1024)                 0         ['dense_6[0][0]']             \n","                                                                                                  \n"," dense_7 (Dense)             (None, 512)                  524800    ['dropout_7[0][0]']           \n","                                                                                                  \n"," dropout_8 (Dropout)         (None, 512)                  0         ['dense_7[0][0]']             \n","                                                                                                  \n"," dense_8 (Dense)             (None, 20)                   10260     ['dropout_8[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 11684744 (44.57 MB)\n","Trainable params: 11684744 (44.57 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n","Epoch 1/10\n","89/89 - 109s - loss: 2.9442 - accuracy: 0.0835 - val_loss: 2.7194 - val_accuracy: 0.1796 - 109s/epoch - 1s/step\n","Epoch 2/10\n","89/89 - 105s - loss: 2.2373 - accuracy: 0.2256 - val_loss: 1.9452 - val_accuracy: 0.3982 - 105s/epoch - 1s/step\n","Epoch 3/10\n","89/89 - 105s - loss: 1.5829 - accuracy: 0.4205 - val_loss: 1.6262 - val_accuracy: 0.5061 - 105s/epoch - 1s/step\n","Epoch 4/10\n","89/89 - 109s - loss: 1.2247 - accuracy: 0.5449 - val_loss: 1.4144 - val_accuracy: 0.5612 - 109s/epoch - 1s/step\n","Epoch 5/10\n","89/89 - 106s - loss: 0.9497 - accuracy: 0.6504 - val_loss: 1.2407 - val_accuracy: 0.6256 - 106s/epoch - 1s/step\n","Epoch 6/10\n","89/89 - 105s - loss: 0.7431 - accuracy: 0.7297 - val_loss: 1.1645 - val_accuracy: 0.6180 - 105s/epoch - 1s/step\n","Epoch 7/10\n","89/89 - 107s - loss: 0.6243 - accuracy: 0.7775 - val_loss: 0.9944 - val_accuracy: 0.6811 - 107s/epoch - 1s/step\n","Epoch 8/10\n","89/89 - 107s - loss: 0.5185 - accuracy: 0.8103 - val_loss: 0.9891 - val_accuracy: 0.6810 - 107s/epoch - 1s/step\n","Epoch 9/10\n","89/89 - 105s - loss: 0.3844 - accuracy: 0.8591 - val_loss: 0.9275 - val_accuracy: 0.6945 - 105s/epoch - 1s/step\n","Epoch 10/10\n","89/89 - 108s - loss: 0.3191 - accuracy: 0.8928 - val_loss: 0.9148 - val_accuracy: 0.7069 - 108s/epoch - 1s/step\n","236/236 [==============================] - 14s 60ms/step\n","              precision    recall  f1-score   support\n","\n","           0       0.58      0.64      0.61       319\n","           1       0.44      0.66      0.53       389\n","           2       0.68      0.60      0.64       394\n","           3       0.50      0.58      0.54       392\n","           4       0.50      0.55      0.52       385\n","           5       0.83      0.44      0.58       395\n","           6       0.73      0.81      0.77       390\n","           7       0.80      0.86      0.83       396\n","           8       0.91      0.80      0.85       398\n","           9       0.90      0.85      0.87       397\n","          10       0.92      0.93      0.93       399\n","          11       0.87      0.81      0.84       396\n","          12       0.61      0.47      0.53       393\n","          13       0.85      0.82      0.84       396\n","          14       0.87      0.79      0.83       394\n","          15       0.66      0.88      0.76       398\n","          16       0.70      0.73      0.71       364\n","          17       0.94      0.82      0.87       376\n","          18       0.58      0.60      0.59       310\n","          19       0.36      0.30      0.33       251\n","\n","    accuracy                           0.71      7532\n","   macro avg       0.71      0.70      0.70      7532\n","weighted avg       0.72      0.71      0.71      7532\n","\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}